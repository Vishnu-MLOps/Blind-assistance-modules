# -*- coding: utf-8 -*-
"""util_depthfm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cjbs6HqnRl7ACcuMfEmkiLG7KJj-vQL_
"""

import os
import math
import torch
import torch.nn as nn
import numpy as np
from einops import repeat


def extract_into_tensor(a, t, x_shape):
  b, *_ = t.shape
  out = a.gather(-1, t)
  return out.reshape(b, *((1,) * (len(x_shape) - 1)))
#a.gather(dim, indices) gathers elements from the last dim with keys from t
'''a = torch..tensor([1, 2, 3, 4]) -->shape(4, )
   t = torch.tensor([0, 3, 1])
   ans = a.gather(-1, t) --> tensor([1, 4, 2])'''
#out now has shape (b, 1, 1, 1)
#Also, when result = out * some_tensor_with_shape_[b, 2, 4, 4], python implicity broadcasts [b, 1, 1, 1] to [b, 2, 4, 4]

def checkpoint(func, inputs, params, flag):
"""
    Evaluate a function without caching intermediate activations, allowing for
    reduced memory at the expense of extra compute in the backward pass.
    :param func: the function to evaluate.
    :param inputs: the argument sequence to pass to `func`.
    :param params: a sequence of parameters `func` depends on but does not
                   explicitly take as arguments.
    :param flag: if False, disable gradient checkpointing.
    """
  if flag:
    args = tuple(inputs) + tuple(params)
    return  CheckpointFunction.apply(func, len(inputs), *args)
  else:
    return func(*inputs)

class CheckpointFunction(torch.autograd.Function):
  @staticmethod
  def foward(ctx, run_function, length, *args):
    ctx.run_function = run_function
    ctx.input_tensors = list(args[:length])
    ctx.input_params = list(args[length:])
    ctx.gpu_autocast_kwargs = {"enabled": torch.is_autocast_enabled(),
                               "dtype": torch.get_autocast_gpu_dtype(),
                               "cache_enabled": torch.is_autocast_cache_enabled()}
    #Stores AMP (Automatic Mixed Precision) settings if autocast is enabled (for memory efficiency and performance)
    with torch.no_grad():
      output_tensors = ctx.run_function(*ctx.input_tensors)
      #During the forward pass, it runs functions without saving memory
    return output_tensors
  '''
  #ctx is a temp object provided by autograd.Function during the forward() pass that lets you store info for the bacward() pass
  #@staticmethod:
  e.g., Class MyClass:
              @staticmethod
              def add(a, b):
                return  a+b
          MyClass.add(2, 3) -->works
          obj = MyClass()
          obj.add(2, 3) -->also works'''
  @staticmethod
  def backward(ctx, *output_grads):
    #ctx --> context object w context stored in the forward pass
    #output_grads --> gradients of loss w.r.t the outputs of the run function
    ctx.input_tensors = [x.detach().requires_grad_(True) for x in ctx.input_tensors]
    with torch.enable_grad(), \
            torch.cuda.amp.autocast(**ctx.gpu_autocast_kwargs):
             # Fixes a bug where the first op in run_function modifies the
            # Tensor storage in place, which is not allowed for detach()'d
            # Tensors.
            shallow_copies = [x.view_as(x) for x in ctx.input_tensors]
            output_tensors = ctx.run_function(*shallow_copies)
    input_grads = torch.autograd.grad(output_tensors,
                                      ctx.input_tensors + ctx.input_params,
                                      output_grads, allow_unused=True)
    del ctx.input_tensors
    del ctx.input_params
    del output_tensors
    return (None, None) + input_grads
'''-In checkpointing, we've lost the original graph from the forward pass because of torch.no_grad
    Therefore, we detach the gradients and then recalculate the garph for the backward pass.
    requires_grad=True for this
   -Shallow copies are created because some models modify inputs in-place, which is
    not allowed on detach()
   -output_tensors reruns the forward pass
   -torch.autograd.grad then manually computes the gradients
   -    ctx.gpu_autocast_kwargs uses mixed precision
   -Mixed precision is a training technique that uses both 16-bit and 32-bit floats to reduce memory usage and accelerate computation without sacrificing model accuracy
   -del is used to manually free up memory
   -return returns to CheckpointFunction.apply(func, len(inputs), *args)--> (None, None, gradients)
   -What we achieved?
        No intermediate activations to compute the gradients are saved, leading to a large memory save'''

def timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False):
    """
    Create sinusoidal timestep embeddings.
    :param timesteps: a 1-D Tensor of N indices, one per batch element.
                      These may be fractional.
    :param dim: the dimension of the output.
    :param max_period: controls the minimum frequency of the embeddings.
    :return: an [N x dim] Tensor of positional embeddings.
    """
    if not repeat_only:
        half = dim // 2
        #If dim = 128, then we want 64 cosine and 64 sine waves
        freqs = torch.exp(
            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half
        ).to(device=timesteps.device) #freqs is moved to the device timesteps is in
        args = timesteps[:, None].float() * freqs[None] #Multiply timesteps with frequencies
        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)
        if dim % 2:
            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)
    else:
        embedding = repeat(timesteps, 'b -> b d', d=dim)
    return embedding
'''
We want to convert a single scalar timestep like t = 3.0
into a vector of real numbers like [0.73, 0.29, -0.88, ..., 0.11]
that captures the position of this timestep in a meaningful way.

This vector is called a positional or timestep embedding.
You want the model to understand how close 3.0 is to 2.0, or how far 3.0 is from 0.0 in a continuous, smooth way.
We need an encoding where:
Nearby timesteps → similar vectors
Far timesteps → different vectors
Periodic patterns (like oscillations over time) are encoded

-Sine and cosine:
  Naturally encode periodicity
  Are bounded between -1 and 1
Goal: Embed(t)=[cos(t⋅f1),cos(t⋅f2),...,cos(t⋅fn),sin(t⋅f1),sin(t⋅f2)

-Say we use dim = 6, so n = 3 frequencies:
  Let’s pick: f0 = 1, f1 = 0.1, f2 = 0.01

  cos(1),cos(0.1),cos(0.01),sin(1),sin(0.1),sin(0.01)
  Each entry reflects how timestep t=1 behaves on a different wave length

-args = timesteps[:, None].float() * freqs[None]
  Here, timesteps is reshaped to [B, 1] and freqs is reshaped to [1, size]
  Pytorch's in-built broadcasting(trick to operate on different sized tensors) converts this to [B, size]'''

def zero_module(module):
  for p in module.parameters():
    p.detach().zero_()
  return module

def scale_module(module, scale):
  for p in module.parameters():
    p.detach().mul_(scale)
  return module

def mean_flat(tensor):
  return tensor.mean(dim=list(range(1, len(tensor.shape))))
  #computes the mean of all non-batch dimension

def normalization(channels):
  return GroupNorm32(32, channels)

class GroupNorm32(nn.GroupNorm):
  def forward(self, x):
    return super().forward(x.float()).type(x.dtype)
    #If the model does mixed precison, then float16 can be too imprecise
    #math operations, so it is temporarily converted to float32 and then change it back

    #super() is used to call a method from the parent class

class SiLU(nn.Module):
  def forward(self, x):
    return x * torch.sigmoid(x)
    #Activation function used in SOTA models

def conv_nd(dims, *args, **kwargs):
  if dims == 1:
    return conv1d(*args, **kwargs)
  elif dims == 2:
    return conv2d(*args, **kwargs)
  elif dims == 3:
    return conv3d(*args, **kwargs)
  raise ValueError(f"unsupported dimensions: {dims}")

def linear(*args, **kwargs):
  return nn.Linear(*args, **kwargs)

def avg_pool_nd(dims, *args, **kwargs):
    """
    Create a 1D, 2D, or 3D average pooling module.
    """
    if dims == 1:
        return nn.AvgPool1d(*args, **kwargs)
    elif dims == 2:
        return nn.AvgPool2d(*args, **kwargs)
    elif dims == 3:
        return nn.AvgPool3d(*args, **kwargs)
    raise ValueError(f"unsupported dimensions: {dims}")



