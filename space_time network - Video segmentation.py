# -*- coding: utf-8 -*-
"""Space-Time.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12M7MqAqpGf3RMyi6Q89bwAFK3hxSfoyW
"""

!wget https://data.vision.ee.ethz.ch/csergi/share/davis/DAVIS-2017-trainval-480p.zip

import zipfile

with zipfile.ZipFile('/kaggle/working/DAVIS-2017-trainval-480p.zip', 'r') as zip_ref:
  zip_ref.extractall('/kaggle/working/train-val')

!ls /kaggle/working/train-val/DAVIS/JPEGImages/480p

import os
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from matplotlib import animation

# Paths
img_dir = "/kaggle/working/train-val/DAVIS/JPEGImages/480p/schoolgirls"
mask_dir = "/kaggle/working/train-val/DAVIS/Annotations/480p/schoolgirls"

# Sorted lists of files
img_files = sorted([f for f in os.listdir(img_dir) if f.endswith('.jpg')])
mask_files = sorted([f for f in os.listdir(mask_dir) if f.endswith('.png')])

# Load all frames into memory
frames = []
for img_file, mask_file in zip(img_files, mask_files):
    img = np.array(Image.open(os.path.join(img_dir, img_file)).convert("RGB"))
    mask = np.array(Image.open(os.path.join(mask_dir, mask_file)))

    overlay = img.copy()
    overlay[mask == 1] = [255, 0, 0]  # Red for mask
    alpha = 0.4
    blended = (overlay * alpha + img * (1 - alpha)).astype(np.uint8)

    frames.append(blended)

# Set up plot
fig, ax = plt.subplots()
im = ax.imshow(frames[0])
ax.axis('off')

def update(frame):
    im.set_data(frame)
    return [im]

# Create animation
anim = animation.FuncAnimation(fig, update, frames=frames, interval=50, blit=True)

# Show as HTML5 video
from IPython.display import HTML
HTML(anim.to_html5_video())

from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init as init
import torch.utils.model_zoo as model_zoo
from torchvision import models
import cv2
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np
import time
import os
import copy

def ToCuda(xs):
  if torch.cuda.is_available():
    if isinstance(xs, list) or isinstance(xs, tuple):
      return [x.cuda() for x in xs]
    else:
      return xs.cuda()
  else:
    return xs

def pad_divide_by(in_list, d, in_size):
  out_list = []
  h, w = in_size
  if h % d > 0:
        new_h = h + d - h % d
  else:
        new_h = h
  if w % d > 0:
      new_w = w + d - w % d
  else:
      new_w = w
  lh, uh = int((new_h-h) / 2), int(new_h-h) - int((new_h-h) / 2)
  lw, uw = int((new_w-w) / 2), int(new_w-w) - int((new_w-w) / 2)
  pad_array = (int(lw), int(uw), int(lh), int(uh))
  for inp in in_list:
      out_list.append(F.pad(inp, pad_array))
  return out_list, pad_array

def overlay_davis(image, mask, colors=[255, 0, 0], cscale=2, alpha=0.4):
  from scipy.ndimage import binary_erosion, binary_dilation

  colors = np.reshape(colors, (-1, 3))
  colors = np.atleast_2d(colors) * cscale

  im_overlay = image.copy()
  object_ids = np.unique(mask)
  for object_id in object_ids[1: ]:
    foreground = image*alpha + np.ones(image.shape)*(1-alpha) * np.array(colors[object_id])
    binary_mask = mask == object_id

    im_overlay[binary_mask] = foreground[binary_mask]
    contours = binary_dilation(binary_mask) ^ binary_mask
    im_overlay[contours, :] = 0

  return im_overlay.astype(image.dtype)

import torch

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(device)

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init as init
import torch.utils.model_zoo as model_zoo
from torchvision import models
import cv2
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np
import math
import time
import os
import argparse
import copy
import sys

class ResBlock(nn.Module):
  def __init__(self, indim, outdim=None, stride=1):
    super(ResBlock, self).__init__()
    if outdim == None:
      outdim = indim
    if indim == outdim and stride == 1:
      self.downsample = None
    else:
      self.downsample = nn.Conv2d(indim, outdim, kernel_size=3, padding=1, stride=stride)

    self.conv1 = nn.Conv2d(indim, outdim, kernel_size=3, padding=1, stride=stride)
    self.conv2 = nn.Conv2d(outdim, outdim, kernel_size=3, padding = 1)

  def forward(self, x):
    r = self.conv1(F.relu(x))
    r = self.conv2(F.relu(r))

    if self.downsample is not None:
      x = self.downsample(x)
    return x + r

class Encoder_M(nn.Module):
  def __init__(self):
    super(Encoder_M, self).__init__()
    self.conv1_m = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
    self.conv1_o = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

    resnet = models.resnet50(pretrained=True)
    self.conv1 = resnet.conv1
    self.bn1 = resnet.bn1
    self.relu = resnet.relu
    self.maxpool = resnet.maxpool

    self.res2 = resnet.layer1
    self.res3 = resnet.layer2
    self.res4 = resnet.layer3

    self.register_buffer('mean', torch.FloatTensor([0.485, 0.456, 0.406]).view(1,3,1,1))
    self.register_buffer('std', torch.FloatTensor([0.229, 0.224, 0.225]).view(1,3,1,1))

  def forward(self, in_f, in_m, in_o):
    f = (in_f - self.mean) / self.std
    m = torch.unsqueeze(in_m, dim=1).float()
    o = torch.unsqueeze(in_o, dim=1).float()

    x = self.conv1(f) + self.conv1_m(m) + self.conv1_o(o)
    x = self.bn1(x)
    c1 = self.relu(x)
    x = self.maxpool(c1)
    r2 = self.res2(x)
    r3 = self.res3(r2)
    r4 = self.res4(r3)
    return r4, r3, r2, c1, f

class Encoder_Q(nn.Module):
  def __init__(self):
    super(Encoder_Q, self).__init__()
    resnet = models.resnet50(pretrained=True)
    self.conv1 = resnet.conv1
    self.bn1 = resnet.bn1
    self.relu = resnet.relu
    self.maxpool = resnet.maxpool

    self.res2 = resnet.layer1
    self.res3 = resnet.layer2
    self.res4 = resnet.layer3
    self.register_buffer('mean', torch.FloatTensor([0.485, 0.456, 0.406]).view(1,3,1,1))
    self.register_buffer('std', torch.FloatTensor([0.229, 0.224, 0.225]).view(1,3,1,1))

  def forward(self, in_f):
    f = (in_f - self.mean) / self.std

    x = self.conv1(f)
    x = self.bn1(x)
    c1 = self.relu(x)   # 1/2, 64
    x = self.maxpool(c1)  # 1/4, 64
    r2 = self.res2(x)   # 1/4, 256
    r3 = self.res3(r2) # 1/8, 512
    r4 = self.res4(r3) # 1/8, 1024
    return r4, r3, r2, c1, f

class Refine(nn.Module):
  def __init__(self, inplanes, planes, scale_factor=2):
    super(Refine, self).__init__()
    self.convFS = nn.Conv2d(inplanes, planes, kernel_size=(3, 3), padding=(1, 1), stride=1)
    self.ResFS = ResBlock(planes, planes)
    self.ResMM = ResBlock(planes, planes)
    self.scale_factor = scale_factor

  def forward(self, f, pm):
    s = self.ResFS(self.convFS(f))
    m = s + F.interpolate(pm, scale_factor=self.scale_factor, mode='bilinear', align_corners=False)
    m = self.ResMM(m)
    return m

class Decoder(nn.Module):
  def __init__(self, mdim):
    super(Decoder, self).__init__()
    self.convFM = nn.Conv2d(1024, mdim, kernel_size=(3, 3), padding=(1, 1), stride=1)
    self.ResMM = ResBlock(mdim, mdim)
    self.RF3 = Refine(512, mdim)
    self.RF2 = Refine(256, mdim)
    self.pred2 = nn.Conv2d(mdim, 2, kernel_size=(3, 3), padding=(1, 1), stride=1)

  def forward(self, r4, r3, r2):
    m4 = self.ResMM(self.convFM(r4))
    m3 = self.RF3(r3, m4)
    m2 = self.RF2(r2, m3)
    p2 = self.pred2(F.relu(m2))
    p = F.interpolate(p2, scale_factor=4, mode='bilinear', align_corners=False)
    return p

class Memory(nn.Module):
    def __init__(self):
        super(Memory, self).__init__()

    def forward(self, m_in, m_out, q_in, q_out):  # m_in: o,c,t,h,w
        B, D_e, T, H, W = m_in.size()
        _, D_o, _, _, _ = m_out.size()

        mi = m_in.view(B, D_e, T*H*W)
        mi = torch.transpose(mi, 1, 2)  # b, THW, emb

        qi = q_in.view(B, D_e, H*W)  # b, emb, HW

        p = torch.bmm(mi, qi) # b, THW, HW
        p = p / math.sqrt(D_e)
        p = F.softmax(p, dim=1) # b, THW, HW

        mo = m_out.view(B, D_o, T*H*W)
        mem = torch.bmm(mo, p) # Weighted-sum B, D_o, HW
        mem = mem.view(B, D_o, H, W)

        mem_out = torch.cat([mem, q_out], dim=1)

        return mem_out, p

class KeyValue(nn.Module):
  def __init__(self, indim, keydim, valdim):
    super(KeyValue, self).__init__()
    self.Key = nn.Conv2d(indim, keydim, kernel_size=(3, 3), padding=(1, 1), stride=1)
    self.Value = nn.Conv2d(indim, valdim, kernel_size=(3, 3), padding=(1, 1), stride=1)

  def forward(self, x):
    return self.Key(x), self.Value(x)


class STM(nn.Module):
  def __init__(self, single_object=True):
    super(STM, self).__init__()
    self.Encoder_M = Encoder_M()
    self.Encoder_Q = Encoder_Q()

    self.KV_M_r4 = KeyValue(1024, keydim=128, valdim=512)
    self.KV_Q_r4 = KeyValue(1024, keydim=128, valdim=512)

    self.Memory = Memory()
    self.Decoder = Decoder(256)
    self.single_object = single_object

  def Pad_memory(self, mems, num_objects, K):
    pad_mems = []
    for mem in mems:
      pad_mem = ToCuda(torch.zeros(1, K, mem.size()[1], 1, mem.size()[2], mem.size()[3]))
      pad_mem[0, 1:num_objects+1, :, 0] = mem
      pad_mems.append(pad_mem)
    return pad_mems

  def memorize(self, frame, masks, num_objects):
    num_objects = num_objects[0].item()
    _, K, H, W = masks.shape

    (frame, masks), pad = pad_divide_by([frame, masks], 16, (frame.size()[2], frame.size()[3]))

    B_list = {'f':[], 'm':[], 'o':[]}
    for o in range(1, num_objects+1):
      B_list['f'].append(frame)
      B_list['m'].append(masks[:, o])
      B_list['o'].append((torch.sum(masks[:, 1:o], dim=1) + \
                          torch.sum(masks[:, o+1:num_objects+1], dim=1)).clamp(0, 1))

    B_ = {}
    for arg in B_list.keys():
      B_[arg] = torch.cat(B_list[arg], dim=0)
    r4, _, _, _, _ = self.Encoder_M(B_['f'], B_['m'], B_['o'])
    k4, v4 = self.KV_M_r4(r4)
    k4, v4 = self.Pad_memory([k4, v4], num_objects=num_objects, K=K)
    return k4, v4

  def Soft_aggregation(self, ps, K):
    num_objects, H, W = ps.shape
    em = ToCuda(torch.zeros(1, K, H, W))
    em[0, 0] = torch.prod(1-ps, dim=0)
    em[0, 1:num_objects+1]= ps
    em = torch.clamp(em, 1e-7, 1-1e-7)
    logit = torch.log((em/(1-em)))
    return logit

  def segment(self, frame, keys, values, num_objects):
    num_objects = num_objects[0].item()
    _, K, keydim, T, H, W = keys.shape
    [frame], pad = pad_divide_by([frame], 16, (frame.size()[2], frame.size()[3]))
    r4, r3, r2, _, _ = self.Encoder_Q(frame)
    k4, v4 = self.KV_Q_r4(r4)

    k4e, v4e = k4.expand(num_objects, -1, -1, -1), v4.expand(num_objects, -1, -1, -1)
    r3e, r2e = r3.expand(num_objects, -1, -1, -1), r2.expand(num_objects, -1, -1, -1)

    m4, viz = self.Memory(keys[0, 1:num_objects+1], values[0, 1:num_objects+1], k4e, v4e)
    logits = self.Decoder(m4, r3e, r2e)
    ps = F.softmax(logits, dim=1)[:, 1]
    #ps is the independant possibility belonging to each object

    logit = self.Soft_aggregation(ps, K)
    if self.single_object:
        logit = logit[:, 1:2, :, :]
    if pad[2]+pad[3] > 0:
      logit = logit[:,:,pad[2]:-pad[3],:]
    if pad[0]+pad[1] > 0:
      logit = logit[:,:,:,pad[0]:-pad[1]]

    return logit

  def forward(self, *args, **kwargs):
    if args[1].dim() > 4:
      return self.segment(*args, **kwargs)
    else:
      return self.memorize(*args, **kwargs)

import os
import os.path as osp
import numpy as np
from PIL import Image

import torch
import torchvision
from torch.utils.data import Dataset
import glob

class Davis_Mo_Test(Dataset):
  def __init__(self, root, train, year, resolution='480p', single_object=True):
    self.root = root
    if year == 2016:
      if train:
        imset='2016/train.txt'
      else:
        imset='2016/val.txt'
    elif year == 2017:
      if train:
        imset='2017/train.txt'
      else:
        imset='2017/val.txt'
    self.mask_dir = os.path.join(root, 'Annotations', resolution)
    self.mask480_dir = os.path.join(root, 'Annotations', '480p')
    self.image_dir = os.path.join(root, 'JPEGImages', resolution)
    _imset_dir = os.path.join(root, 'ImageSets')
    _imset_f = os.path.join(_imset_dir, imset)

    self.videos = []
    self.num_frames = {}
    self.num_objects = {}
    self.shape = {}
    self.size_480p = {}
    with open(_imset_f, 'r') as lines:
      for line in lines:
        _video = line.rstrip('\n')
        self.videos.append(_video)
        self.num_frames[_video] = len(glob.glob(os.path.join(self.image_dir, _video, '*.jpg')))
        _mask = np.array(Image.open(os.path.join(self.mask_dir, _video, '00000.png')).convert('P'))
        #In segmentation, P mode means pixel values are integer class IDs instead of RGB colors.
        self.num_objects[_video] = np.max(_mask)
        self.shape[_video] = np.shape(_mask)
        _mask480 = np.array(Image.open(os.path.join(self.mask480_dir, _video, '00000.png')).convert('P'))
        self.size_480p[_video] = np.shape(_mask480)

    if single_object:
        self.K = 2
    else:
        self.K = 11

    self.single_object = single_object

  def __len__(self):
    return len(self.videos)

  def To_onehot(self, mask):
    M = np.zeros((self.K, mask.shape[0], mask.shape[1]), dtype=np.uint8)
    for k in range(self.K):
      M[k] = (mask == k).astype(np.uint8)
    return M

  def All_to_onehot(self, masks):
    Ms = np.zeros((self.K, masks.shape[0], masks.shape[1], masks.shape[2]), dtype=np.uint8)
    for n in range(masks.shape[0]):
      Ms[:, n] = self.To_onehot(masks[n])

    return Ms

  def __getitem__(self, index):
    video = self.videos[index]
    info = {}
    info['name'] = video
    info['num_frames'] = self.num_frames[video]
    info['size_480p'] = self.size_480p[video]

    N_frames = np.empty((self.num_frames[video], ) + self.shape[video]+(3,), dtype=np.float32)
    N_masks = np.empty((self.num_frames[video], ) + self.shape[video], dtype=np.uint8)
    for f in range(self.num_frames[video]):
      img_file = os.path.join(self.image_dir, video, '{:05d}.jpg'.format(f))
      #This formats the frame index f into a 5‑digit zero‑padded number.
      #E.g., '00001.jpg'
      N_frames[f] = np.array(Image.open(img_file).convert('RGB'))/255.
      try:
        mask_file = os.path.join(self.mask_dir, video, '{:05d}.png'.format(f))
        N_masks[f] = np.array(Image.open(mask_file).convert('P'), dtype=np.uint8)
      except:
        N_masks[f] = 255

    Fs = torch.from_numpy(np.transpose(N_frames.copy(), (3, 0, 1, 2)).copy()).float()
    if self.single_object:
      N_masks = (N_masks > 0.5).astype(np.uint8) * (N_masks < 255).astype(np.uint8)
      Ms = torch.from_numpy(self.All_to_onehot(N_masks).copy()).float()
      num_objects = torch.LongTensor([int(1)])
      return Fs, Ms, num_objects, info
    else:
      Ms = torch.from_numpy(self.All_to_onehot(N_masks).copy()).float()
      num_objects = torch.LongTensor([int(self.num_objects[video])])
      return Fs, Ms, num_objects, info

import random
import torch.nn.functional as F
from torch.utils.data import DataLoader
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from IPython.display import HTML
from torch.utils.data import ConcatDataset

def train_batch(model, optimizer, criterion, data):
  model.train()
  Fs, Ms, num_objects, info = [x.to(device) if torch.is_tensor(x) else x for x in data]
  optimizer.zero_grad()

  B, _, T, H, W = Fs.shape
  mem_idx = random.randint(0, T - 2)          # Memory frame (not the last one)
  query_idx = random.randint(mem_idx + 1, T - 1)  # Query must be later than memory

  mem_frame = Fs[:, :, mem_idx]   # (B, 3, H, W)
  mem_mask  = Ms[:, :, mem_idx]   # (B, K, H, W)

  query_frame = Fs[:, :, query_idx]
  query_mask = Ms[:, :, query_idx]

  keys, values = model(mem_frame, mem_mask, num_objects)
  logit = model(query_frame, keys, values, num_objects)
  target = query_mask[:, 1:2, :, :].float()
  loss = criterion(logit, target)
  loss.backward()
  optimizer.step()
  return loss.item()

@torch.no_grad()
def validate_batch(model, data, criterion, plot_video=True):
    Fs, Ms, num_objects, info = [x.to(device) if torch.is_tensor(x) else x for x in data]
    B, _, T, H, W = Fs.shape

    model.eval()
    total_loss = 0.0
    iou_scores = []

    # To store frames for plotting
    frames_to_plot = []

    with torch.no_grad():
        # Initialize memory with first frame's GT mask
        keys, values = model(Fs[:, :, 0], Ms[:, :, 0], num_objects)
        to_memorize = [0] + list(range(5, T, 5))
        for t in range(1, T):
            # Segment current frame
            logits = model(Fs[:, :, t], keys, values, num_objects)
            target = Ms[:, 1:2, t].float()
            loss = criterion(logits, target)
            total_loss += loss.item()

            # mIoU
            if logits.shape[1] == 1:  # Binary segmentation
                pred_mask = (torch.sigmoid(logits) > 0.5).long()
            else:  # Multi-class segmentation
                pred_mask = torch.argmax(torch.softmax(logits, dim=1), dim=1)

# Resize to (H, W) = GT size
            H, W = Fs.shape[3], Fs.shape[4]
            #pred_mask = pred_mask.unsqueeze(1)  # [B, 1, h, w]
            pred_mask = F.interpolate(pred_mask.float(), size=(H, W), mode='nearest').long()
            pred_mask = pred_mask[:, 0, :, :]  # safely extract [B, H, W]

# Ground truth mask
            if logits.shape[1] == 1:
                gt_mask = Ms[:, 0, t, :, :].long()
            else:
                gt_mask = torch.argmax(Ms[:, :, t, :, :], dim=1)

            intersection = (pred_mask & gt_mask).float().sum((1, 2))
            union = (pred_mask | gt_mask).float().sum((1, 2))
            iou = (intersection / (union + 1e-6)).mean().item()
            iou_scores.append(iou)

            # Collect frames for video plotting
            if plot_video:
                rgb_frame = (Fs[0, :, t].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
                mask_np = pred_mask[0].cpu().numpy()

                overlay = rgb_frame.copy()
                overlay[mask_np == 1] = [255, 0, 0]  # Red overlay for object
                alpha = 0.4
                blended = (overlay * alpha + rgb_frame * (1 - alpha)).astype(np.uint8)
                frames_to_plot.append(blended)

            # Update memory with GT masks for validation
            if t in to_memorize:
                k, v = model(Fs[:, :, t], Ms[:, :, t], num_objects)
                keys = torch.cat([keys, k], dim=3)
                values = torch.cat([values, v], dim=3)

    avg_loss = total_loss / (T - 1)
    miou = sum(iou_scores) / len(iou_scores)

    # Plot video if requested
    if plot_video and len(frames_to_plot) > 0:
        fig, ax = plt.subplots()
        im = ax.imshow(frames_to_plot[0])
        ax.axis('off')

        def update(frame):
            im.set_data(frame)
            return [im]

        anim = animation.FuncAnimation(fig, update, frames=frames_to_plot, interval=100, blit=True)
        plt.close(fig)
        display(HTML(anim.to_html5_video()))

    return avg_loss, miou


root = '/kaggle/working/train-val/DAVIS'

train_ds_1 = Davis_Mo_Test(root, train=True, year=2016)
train_ds_2 = Davis_Mo_Test(root, train=True, year=2017)
val_ds_1 = Davis_Mo_Test(root, train=False, year=2016)
val_ds_2 = Davis_Mo_Test(root, train=False, year=2017)

train_ds = ConcatDataset([train_ds_1, train_ds_2])
val_ds   = ConcatDataset([val_ds_1, val_ds_2])

train_dl = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=2, pin_memory=True)
val_dl = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)

model = STM(single_object=True)
model = model.to(device)

if torch.cuda.device_count() > 1:
  print(f"Using {torch.cuda.device_count()} GPUs")
  model = nn.DataParallel(model)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
criterion = torch.nn.BCEWithLogitsLoss()

from tqdm import tqdm

train_losses = []
val_losses = []
epochs = 10

for epoch in range(epochs):
    print(f"\nEpoch {epoch+1}")
    N = len(train_dl)
    tr_batch_loss = 0
    for i, data in enumerate(tqdm(train_dl)):
        tr_batch_loss += train_batch(model, optimizer, criterion, data)
    tr_epoch_loss = tr_batch_loss/N
    train_losses.append(tr_epoch_loss)
    print(f"\nTrain Loss: {tr_epoch_loss}")

    N = len(val_dl)
    val_batch_loss = 0
    val_batch_iou = 0

    for i, data in enumerate(tqdm(val_dl, desc=f"Epoch {epoch+1} [Val]")):
        plot_video_flag = (i == 0)  # Only first batch in epoch
        batch_loss, batch_iou = validate_batch(model, data, criterion, plot_video=plot_video_flag)
        val_batch_loss += batch_loss
        val_batch_iou += batch_iou

    val_epoch_loss = val_batch_loss / len(val_dl)
    val_epoch_iou = val_batch_iou / len(val_dl)
    print(f"Valid loss: {val_epoch_loss:.4f}, mIoU: {val_epoch_iou:.4f}")





