# -*- coding: utf-8 -*-
"""Caption_epoch 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cy0fj596Q_PkvU_BJh35WMCynkNbMPJ9
"""

!pip uninstall torch torchvision torchtext -y
!pip uninstall numpy -y
!pip install torch torchvision torchtext --index-url https://download.pytorch.org/whl/cu121
!pip install numpy

import torch

device = 'cuda' if torch.cuda.is_available() else 'cpu'
device

import os
from google.colab import drive

drive.mount('/content/drive')

checkpoint_dir = '/content/drive/My Drive/checkpoints'
os.makedirs(checkpoint_dir, exist_ok=True)

from torchtext.vocab import build_vocab_from_iterator

!wget https://vizwiz.cs.colorado.edu/VizWiz_final/images/train.zip

!wget https://vizwiz.cs.colorado.edu/VizWiz_final/images/val.zip

!wget https://vizwiz.cs.colorado.edu/VizWiz_final/images/test.zip

!wget https://vizwiz.cs.colorado.edu/VizWiz_final/caption/annotations.zip

import zipfile

with zipfile.ZipFile('/content/train.zip', 'r') as zip_ref:
    zip_ref.extractall('train_images')

with zipfile.ZipFile('/content/val.zip', 'r') as zip_ref:
    zip_ref.extractall('val_images')

with zipfile.ZipFile('/content/test.zip', 'r') as zip_ref:
    zip_ref.extractall('test_images')

with zipfile.ZipFile('/content/annotations.zip', 'r') as zip_ref:
    zip_ref.extractall('train_annotations')

import json

train_json_path = '/content/train_annotations/annotations/train.json'
with open(train_json_path, 'r') as f:
    train_data = json.load(f)

val_json_path = '/content/train_annotations/annotations/val.json'
with open(val_json_path, 'r') as f:
    val_data = json.load(f)

test_json_path = '/content/train_annotations/annotations/test.json'
with open(test_json_path, 'r') as f:
    test_data = json.load(f)

test_data.keys()

val_data.keys()

from tqdm import tqdm
import pandas as pd

tr_img_ls = []
for ix, item in tqdm(enumerate(train_data['images'])):
  x = pd.DataFrame.from_dict(item, orient='index').T
  tr_img_ls.append(x)
tr_img_ls = pd.concat(tr_img_ls)

val_img_ls = []
for ix, item in tqdm(enumerate(val_data['images'])):
  x = pd.DataFrame.from_dict(item, orient='index').T
  val_img_ls.append(x)
val_img_ls = pd.concat(val_img_ls)

test_img_ls = []
for ix, item in tqdm(enumerate(test_data['images'])):
  x = pd.DataFrame.from_dict(item, orient='index').T
  test_img_ls.append(x)
test_img_ls = pd.concat(test_img_ls)

tr_img_ls.rename(columns={'id':'image_id'}, inplace=True)
val_img_ls.rename(columns={'id':'image_id'}, inplace=True)
test_img_ls.rename(columns={'id':'image_id'}, inplace=True)

tr_ann_ls = []
for ix, item in enumerate(tqdm(train_data['annotations'])):
  x = pd.DataFrame.from_dict(item, orient='index').T
  tr_ann_ls.append(x)
tr_ann_ls = pd.concat(tr_ann_ls)

val_ann_ls = []
for ix, item in enumerate(tqdm(val_data['annotations'])):
  x = pd.DataFrame.from_dict(item, orient='index').T
  val_ann_ls.append(x)
val_ann_ls = pd.concat(val_ann_ls)

tr_ann_ls.info()

train_merged = pd.merge(tr_img_ls, tr_ann_ls, on='image_id', how='left')
val_merged = pd.merge(val_img_ls, val_ann_ls, on='image_id', how='left')

merged = pd.concat([train_merged, val_merged], axis=0, ignore_index=True)
merged.info()

merged.head()

merged[merged['file_name'].str.contains('val')]

merged = merged[(merged['is_rejected'] == False)]
merged.info()

import numpy as np

merged['train'] = np.random.choice([True, False], size=len(merged), p=[0.95, 0.05])



from collections import Counter, defaultdict
from torchtext.vocab import Vocab

class VocabWrapper:
    def __init__(self, tokens, specials=['<pad>', '<start>', '<end>', '<unk>']):
        flat_tokens = [tok for seq in tokens for tok in seq]

        # Count frequency
        counter = Counter(flat_tokens)

        # Create vocab with specials
        self.vocab = Vocab(counter, specials=specials)

        # Index-to-string
        self.itos = self.vocab.itos
        self.stoi = defaultdict(lambda: self.vocab['<unk>'])
        for i, token in enumerate(self.itos):
            self.stoi[token] = i

        # Make sure <pad> is index 0
        if self.itos[0] != '<pad>':
            self._force_pad_at_zero()

    def _force_pad_at_zero(self):
        pad_idx = self.itos.index('<pad>')
        self.itos[pad_idx], self.itos[0] = self.itos[0], self.itos[pad_idx]
        self.stoi = defaultdict(lambda: self.vocab['<unk>'])
        for i, token in enumerate(self.itos):
            self.stoi[token] = i

# Your tokenization and vocab build
all_captions = merged[merged['train']]['caption'].tolist()
tokenized = [[w for w in c.lower().split()] for c in all_captions]  # no start/end added yet

vocab = VocabWrapper(tokenized)

import torch

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

import numpy as np

from torchvision import transforms
from torch.utils.data import Dataset
from PIL import Image
import cv2

class CaptioningData(Dataset):

  def __init__(self, df, vocab, train_root=None, val_root=None):

    self.grouped = df.groupby("image_id")
    self.image_ids = list(self.grouped.groups.keys())

    self.samples = []
    for image_id in self.image_ids:
      captions = self.grouped.get_group(image_id)
      row = captions.iloc[0]  # first row only
      self.samples.append({
        'image_id': image_id,
        'caption': row.caption,
        'caption_idx': 0,
        'file_name': row.file_name
      })


    self.train_root = train_root
    self.val_root = val_root
    self.vocab = vocab
    self.transform = transforms.Compose([
                transforms.Resize(224),
                transforms.CenterCrop(224),
                transforms.ColorJitter(0.3),
                transforms.RandomHorizontalFlip(),
                transforms.ToTensor(),
                transforms.Normalize((0.485, 0.456, 0.406),
                                     (0.229, 0.224, 0.225))
            ])

  def __len__(self):
    return len(self.samples)

  def __getitem__(self, idx):
    sample = self.samples[idx]
    image_id = sample['image_id']
    caption = sample['caption']
    caption_idx = sample['caption_idx']
    file_name = sample['file_name']

    img = cv2.imread(f"{self.train_root}/{file_name}") if 'train' in file_name else cv2.imread(f"{self.val_root}/{file_name}")
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB
    img = Image.fromarray(img)
    transform = self.transform
    image = transform(img)

    tokens = str(caption).lower().split()
    target = []
    target.append(self.vocab.stoi['<start>'])
    target.extend(self.vocab.stoi[token] for token in tokens)
    target.append(self.vocab.stoi['<end>'])
    target = torch.Tensor(target).long()
    return image, target, caption

  def choose(self):
    return self[np.random.randint(len(self))]

  def collate_fn(self, data):
    data.sort(key=lambda x: len(x[1]), reverse=True)
    images, targets, captions = zip(*data)
    images = torch.stack(images, 0)
    lengths = [len(tar) for tar in targets]
    _targets = torch.zeros(len(captions), max(lengths)).long()
    for i, tar in enumerate(targets):
      end = lengths[i]
      _targets[i, :end] = tar[:end]
    return images.to(device), _targets.to(device), torch.tensor(lengths).long().to(device)

train_ds = CaptioningData(train_root='/content/train_images/train', val_root='/content/val_images/val',
                          df=merged[merged['train']], vocab=vocab)
val_ds = CaptioningData(train_root='/content/train_images/train', val_root='/content/val_images/val',
                          df=merged[~merged['train']], vocab=vocab)

from torch.utils.data import DataLoader

train_dl = DataLoader(train_ds, batch_size=32, collate_fn=train_ds.collate_fn)
val_dl = DataLoader(val_ds, batch_size=32, collate_fn=val_ds.collate_fn)
len(train_dl), len(val_dl)

from torch.nn.utils.rnn import pack_padded_sequence
from torchvision import models
import torch.nn as nn

class Encoder(nn.Module):
  def __init__(self, embed_size):
    super(Encoder, self).__init__()
    resnet = models.resnet152(pretrained=True)
    modules = list(resnet.children())[:-1]
    self.resnet = nn.Sequential(*modules)
    self.linear = nn.Linear(resnet.fc.in_features, embed_size)
    self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)

  def forward(self, images):
    with torch.no_grad():
      features = self.resnet(images)
    features = features.reshape(features.size(0), -1)
    features = self.bn(self.linear(features))
    return features

encoder = Encoder(256).to(device)

!pip install torch_summary
from torchsummary import summary
summary(encoder, torch.zeros(32, 3, 224, 224).to(device))

class Decoder(nn.Module):
  def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=80):
    super(Decoder, self).__init__()
    self.embed = nn.Embedding(vocab_size, embed_size)
    self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)
    self.linear = nn.Linear(hidden_size, vocab_size)
    self.max_seq_length = max_seq_length

  def forward(self, features, captions, lengths):
    embeddings = self.embed(captions)
    embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)
    packed = pack_padded_sequence(embeddings, lengths.cpu(), batch_first=True)
    outputs, _ = self.lstm(packed)
    outputs = self.linear(outputs[0])
    return outputs

  def predict(self, features, states=None):
    sampled_ids = []
    inputs = features.unsqueeze(1)
    for i in range(self.max_seq_length):
      hiddens, states = self.lstm(inputs, states)
      outputs = self.linear(hiddens.squeeze(1))
      _, predicted = outputs.max(1)
      sampled_ids.append(predicted)
      inputs = self.embed(predicted)
      inputs = inputs.unsqueeze(1)

    sampled_ids = torch.stack(sampled_ids, 1)
    sentences = []
    for sampled_id in sampled_ids:
      sampled_id = sampled_id.cpu().numpy()
      sampled_caption = []
      for word_id in sampled_id:
        word = vocab.itos[word_id]
        sampled_caption.append(word)
        if word == '<end>':
          break
      sentence = ' '.join(sampled_caption)
      sentences.append(sentence)
    return sentences

def train_batch(data, encoder, decoder, optimizer,criterion):
  encoder.train()
  decoder.train()
  images, captions, lengths = data
  images = images.to(device)
  captions = captions.to(device)
  targets = pack_padded_sequence(captions, lengths.cpu(), batch_first=True)[0]
  features = encoder(images)
  outputs = decoder(features, captions, lengths)
  loss = criterion(outputs, targets)
  decoder.zero_grad()
  encoder.zero_grad()
  loss.backward()
  optimizer.step()
  return loss.item()

@torch.no_grad()
def validate_batch(data, encoder, decoder, criterion):
  encoder.eval()
  decoder.eval()
  images, captions, lengths = data
  images = images.to(device)
  captions = captions.to(device)
  targets = pack_padded_sequence(captions, lengths.cpu(), batch_first=True)[0]
  features = encoder(images)
  outputs = decoder(features, captions, lengths)
  loss = criterion(outputs, targets)
  return loss.item()

encoder = Encoder(256).to(device)
decoder = Decoder(256, 512, len(vocab.itos), 1).to(device)
criterion = nn.CrossEntropyLoss()
params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())
optimizer = torch.optim.AdamW(params, lr=1e-3)

def load_image(image_path, transform=None):
    image = Image.open(image_path).convert('RGB')
    image = image.resize((224, 224), Image.LANCZOS)
    if transform is not None:
        tfm_image = transform(image)[None]
    return image, tfm_image

def load_image_and_predict(image_path):
  transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(
        (0.485, 0.456, 0.406),
        (0.229, 0.224, 0.225)
    )
])

  org_image, tfm_image = load_image(image_path, transform)
  image_tensor = tfm_image.to(device)
  encoder.eval()
  decoder.eval()
  feature = encoder(image_tensor)
  sentence = decoder.predict(feature)[0]
  return org_image, sentence

import matplotlib.pyplot as plt
epochs = 10
epoch = 1

train_losses = []
val_losses = []

for epoch in range(epoch, epochs+1):
  print(f"\nEpoch {epoch}")
  if epoch == 5:
    optimizer = torch.optim.AdamW(params, lr=1e-4)
  N = len(train_dl)
  tr_batch_loss = 0
  for i, data in enumerate(tqdm(train_dl)):
    tr_batch_loss += train_batch(data, encoder, decoder, optimizer, criterion)
  tr_epoch_loss = tr_batch_loss/N
  train_losses.append(tr_epoch_loss)
  print(f"\nTrain Loss: {tr_epoch_loss}")

  if epoch % 1 == 0:
    checkpoint_path = os.path.join(checkpoint_dir, f"model_checkpoint_{epoch}.pt")
    torch.save({
        'epoch': epoch,  # current epoch number
        'model_state_dict': encoder.state_dict(),  # save model weights
        'decoder_state_dict': decoder.state_dict(),  # save decoder weights if separate
        'optimizer_state_dict': optimizer.state_dict(),  # save optimizer state
        'train_losses': train_losses,  # any lists you want to keep (optional)
        'val_losses': val_losses,
      }, checkpoint_path)

  N = len(val_dl)
  val_batch_loss = 0
  for i, data in enumerate(tqdm(val_dl)):
    val_batch_loss += validate_batch(data, encoder, decoder, criterion)
  val_epoch_loss = val_batch_loss/N
  val_losses.append(val_epoch_loss)
  print(f"\nValid loss: {val_epoch_loss}")


  predicted_img, sent = load_image_and_predict('/content/train_images/train/VizWiz_train_00023907.jpg')
  plt.imshow(predicted_img)
  print(sent)

!ls /content/train_images/train

predicted_img, sent = load_image_and_predict('/content/train_images/train/VizWiz_train_00015969.jpg')
plt.imshow(predicted_img)
print(sent)

import matplotlib.pyplot as plt
predicted_img, sent = load_image_and_predict('/content/train_images/train/VizWiz_train_00007975.jpg')
plt.imshow(predicted_img)
print(sent)

